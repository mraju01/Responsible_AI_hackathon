{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c4ad0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bharath\n",
      "[nltk_data]     Srinivasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Bharath\n",
      "[nltk_data]     Srinivasan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ced68",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8a26b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1=pd.read_csv(\"cyberbullying_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a86239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7998\n",
       "age                    7992\n",
       "gender                 7973\n",
       "ethnicity              7961\n",
       "not_cyberbullying      7945\n",
       "other_cyberbullying    7823\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98226b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(df):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lambda x: x.translate(translator))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f7493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converToLowerCase(df):\n",
    "    df[\"tweet_text\"]=df[\"tweet_text\"].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc4f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnwantedCharacters(df):\n",
    "    cleaned_tweets=list()\n",
    "    for tweet in df['tweet_text']:\n",
    "        # Remove URLs\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        # Remove mentions\n",
    "        tweet = re.sub(r\"@[A-Za-z0-9]+\", \"\", tweet)\n",
    "        # Remove hashtags\n",
    "        tweet = re.sub(r\"#[A-Za-z0-9]+\", \"\", tweet)\n",
    "        # Remove special characters\n",
    "        tweet = re.sub(r\"[^a-zA-Z0-9]+\", \" \", tweet)\n",
    "        cleaned_tweets.append(tweet)\n",
    "    df=df.drop(\"tweet_text\",axis=1)\n",
    "    df['tweet_text']=cleaned_tweets\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d199dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEmojis(df):\n",
    "    deemoji=list()\n",
    "    for i in df['tweet_text']:\n",
    "        deemoji.append(emoji.demojize(i))\n",
    "    df.drop(\"tweet_text\",axis=1)\n",
    "    df['tweet_text']=deemoji\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b912b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(data):\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    data['tweet_text'] = data['tweet_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c778801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_df(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ec1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_column(dataframe):\n",
    "    # create an empty list to store the lemmatized values\n",
    "    lemmatized_values = []\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # iterate over the values in the column\n",
    "    for value in dataframe[\"tweet_text\"]:\n",
    "        # create a spaCy doc object for the value\n",
    "        doc = nlp(value)\n",
    "        \n",
    "        # lemmatize each token in the doc and join them back into a string\n",
    "        lemmatized_words = [token.lemma_ for token in doc]\n",
    "        lemmatized_value = ' '.join(lemmatized_words)\n",
    "        \n",
    "        # append the lemmatized value to the list\n",
    "        lemmatized_values.append(lemmatized_value)\n",
    "    \n",
    "    # create a new column in the dataframe with the lemmatized values\n",
    "    dataframe['lemmatized_' + \"temp\"] = lemmatized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e31cafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned1=removeStopWords(removeEmojis(removeUnwantedCharacters(removePunctuations(converToLowerCase(data1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be079bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize_column(data_cleaned1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57374827",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned1.to_csv(\"Lemmatized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c6569ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned1=data_cleaned1.drop(\"tweet_text\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42b7ef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lemmatized_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>word katandandre food crapilicious mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>xochitlsuckkks classy whore red velvet cupcake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>jasongio meh p thank head concern another angr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>rudhoeenglish isis account pretend kurdish acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>black ppl be not expect anything depend anythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>turner withhold disappointment turner call cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>swear god dumb nigger bitch get bleach hair re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>yea fuck rt therealexel you re nigger fucking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>bro u get to chill rt chillshrammy dog fuck kp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cyberbullying_type                                    lemmatized_temp\n",
       "0      not_cyberbullying             word katandandre food crapilicious mkr\n",
       "1      not_cyberbullying  aussietv white mkr theblock imacelebrityau tod...\n",
       "2      not_cyberbullying     xochitlsuckkks classy whore red velvet cupcake\n",
       "3      not_cyberbullying  jasongio meh p thank head concern another angr...\n",
       "4      not_cyberbullying  rudhoeenglish isis account pretend kurdish acc...\n",
       "...                  ...                                                ...\n",
       "47687          ethnicity  black ppl be not expect anything depend anythi...\n",
       "47688          ethnicity  turner withhold disappointment turner call cou...\n",
       "47689          ethnicity  swear god dumb nigger bitch get bleach hair re...\n",
       "47690          ethnicity  yea fuck rt therealexel you re nigger fucking ...\n",
       "47691          ethnicity  bro u get to chill rt chillshrammy dog fuck kp...\n",
       "\n",
       "[47692 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a9c5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   word katandandre food crapilicious mkr\n",
       "1        aussietv white mkr theblock imacelebrityau tod...\n",
       "2           xochitlsuckkks classy whore red velvet cupcake\n",
       "3        jasongio meh p thank head concern another angr...\n",
       "4        rudhoeenglish isis account pretend kurdish acc...\n",
       "                               ...                        \n",
       "47687    black ppl be not expect anything depend anythi...\n",
       "47688    turner withhold disappointment turner call cou...\n",
       "47689    swear god dumb nigger bitch get bleach hair re...\n",
       "47690    yea fuck rt therealexel you re nigger fucking ...\n",
       "47691    bro u get to chill rt chillshrammy dog fuck kp...\n",
       "Name: lemmatized_temp, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens=data_cleaned1['lemmatized_temp']\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded5ea1",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e56eb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text_tokens)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5bc391c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd\\nimport numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 404\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = np.max(list(map(lambda x: len(x), sequences)))\n",
    "\n",
    "print(\"Max sequence length:\", max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc341315",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ba563",
   "metadata": {},
   "source": [
    "## Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f15b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "data_cleaned1['cyberbullying_type']=le.fit_transform(data_cleaned1['cyberbullying_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3aac01",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "928ed232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(inputs, data_cleaned1['cyberbullying_type'], train_size=0.7, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553ba8f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb13c085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 404)]             0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 404, 64)           640000    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 25856)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               3309696   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,966,982\n",
      "Trainable params: 3,966,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(train_inputs.shape[1],))\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=10000,\n",
    "    output_dim=64\n",
    ")(inputs)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(embedding)\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(128, activation='relu')(flatten)\n",
    "dense_2 = tf.keras.layers.Dense(128, activation='relu')(dense_1)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(6, activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70276292",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "inputs = tf.keras.Input(shape=(max_seq_length,))\n",
    "\n",
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_seq_length\n",
    ")(inputs)\n",
    "\n",
    "gru = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.GRU(128, return_sequences=True)\n",
    ")(embedding)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(gru)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(flatten)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61cad87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "835/835 [==============================] - 32s 38ms/step - loss: 0.7065 - accuracy: 0.6919 - val_loss: 0.4223 - val_accuracy: 0.8188\n",
      "Epoch 2/100\n",
      "835/835 [==============================] - 32s 39ms/step - loss: 0.3423 - accuracy: 0.8558 - val_loss: 0.4145 - val_accuracy: 0.8255\n",
      "Epoch 3/100\n",
      "835/835 [==============================] - 31s 37ms/step - loss: 0.2473 - accuracy: 0.8983 - val_loss: 0.4660 - val_accuracy: 0.8240\n",
      "Epoch 4/100\n",
      "835/835 [==============================] - 31s 37ms/step - loss: 0.1780 - accuracy: 0.9278 - val_loss: 0.5389 - val_accuracy: 0.8153\n",
      "Epoch 5/100\n",
      "835/835 [==============================] - 31s 38ms/step - loss: 0.1323 - accuracy: 0.9465 - val_loss: 0.6208 - val_accuracy: 0.8055\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b127662",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c72cbf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.07%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy: {:.2f}%\".format(model.evaluate(test_inputs, test_labels, verbose=0)[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e1afbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd\\nimport numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "----------------------\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                age       0.96      0.97      0.97      2378\n",
      "          ethnicity       0.97      0.97      0.97      2443\n",
      "             gender       0.86      0.87      0.87      2406\n",
      "  not_cyberbullying       0.60      0.57      0.59      2370\n",
      "other_cyberbullying       0.62      0.63      0.62      2324\n",
      "           religion       0.94      0.97      0.95      2387\n",
      "\n",
      "           accuracy                           0.83     14308\n",
      "          macro avg       0.83      0.83      0.83     14308\n",
      "       weighted avg       0.83      0.83      0.83     14308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "clr = classification_report(test_labels, y_pred, target_names=['age',\n",
    " 'ethnicity',\n",
    " 'gender',\n",
    " 'not_cyberbullying',\n",
    " 'other_cyberbullying',\n",
    " 'religion'])\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99244826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d70ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
